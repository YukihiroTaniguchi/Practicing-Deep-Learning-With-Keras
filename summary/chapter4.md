#### chapter4 まとめ  
##### 学習の種類
教師あり学習  
教師なし学習  
自己学習  
強化学習  

##### 分類と回帰の用語
|  用語  |  意味  |
| :---: |:--- |
|サンプル、入力|1つのデータポイント|
|予測値または入力|モデルから返される値|
|目的値|真の値、予測すべき値|
|予測誤差<br>損失値|予測値と目的値の距離の目安になる指標|
|クラス|分類問題の分類されるそれぞれの値|
|ラベル|クラスのインスタンス|
|グラウンドトルース<br>アノテーション|データセットのすべての目的値|
|二値分類|各入力サンプルを2つに分ける|
|多クラス分類|各入力サンプルを3つ以上に分ける|
|多ラベル分類|各入力サンプルに複数のラベルを割り当てる|
|スカラー回帰|目的値が連続するスカラー値となるタスク|
|ベクトル回帰|目的値が連続ベクトルとなるタスク|
|バッチ|モデルによって同時に処理されるサンプルの小さな集合<br>2の累乗|

##### モデルの評価を行う検証方法
モデルの訓練 : 訓練データ  
モデルの評価 : 検証データ  
モデルのテスト : テストデータ

ハイパーパラメータのチューニング際に  
検証データをモデルの訓練に使用してしまうと情報の漏れが起こる  
-> 検証データを訓練に使えないが、データが少ない  
-> 評価手法を変える

###### 1. (単純な)ホールドアウト法
訓練データセットと検証データセットに分けるだけ

###### 2. k分割交差検証
利用可能なデータが少ないとき
```python
k = 4
num_validation_samples = len(data) // k
np.random.shuffle(data)
validataion_scores = []

for fold in range(k):
    validation_data = data[num_validation_samples * fold:
                           num_validation_samples * (fold + 1)]
    training_data = data[:num_validation_samples * fold] + data[num_validation_samples * (fold + 1):]

    model = get_model()
    model.train(training_data)

    validation_score = model.evaluate(validation_data)
    validation_scores.append(validation_score)

validation_score = np.average(validation_scores)
model = get_model()
model.train(data)
test_score = model.evaluate(test_data)
```
###### 1. シャッフルに基づく反復的なk分割交差検証
利用可能なデータが少ないとき  
\+  
モデルをできるだけ正確に評価しなければならない場合
k分割交差検証をP回行う
1度のk分割交差検証のたびにデータをシャッフルする

##### モデルの評価の際の注意点
* データの典型性  
->サンプルが昇順だったときにそのまま分割しても意味がない

* 時間の矢  
-> 未来を予測する際はランダムしてはならない  
-> 時間の漏れを作ってしまう  
-> テストデータが常に訓練データより未来でなければならない

* データの冗長性
-> データセットに同じデータポイントが複数出現しているときにシャッフルし、分割することで訓練データと検証データが重複することがある
-> 検証データで訓練することになってしまう
-> 訓練データセットと検証データセットが互いに素であることを確認する


##### 入力データと目的値の準備
###### データ前処理
* データのベクトル化

* 値の正規化  
 -> 小さな値を取る  
 -> 種類が同じである

 ***->平均が0になるように各特徴量を個別に正規化する***  
 ***->標準偏差が1になるように各特徴量を個別に正規化する***

* 欠測値の処理  
 -> テストデータに欠測値が含まれている場合  
 -> 訓練サンプルで欠測値を人工的に作成すべき
 -> 特徴量を一部削除する

###### 特徴エンジニアリング
- よい特徴量があると、リソースの消費を抑えた上で、問題をより的確に解決できる

- よい特徴量があると、問題をずっと少ないデータで解決できる


##### 過学習と学習不足
* 最適化  
-> 訓練データでの性能をできるだけ高めるためにモデルを調整するプロセス

* 汎化  
->  学習済みのモデルを全く新しいデータに適用したときの性能がどれくらいよいかを表す

過学習を回避する方法
1. 訓練データを増やす

2. モデルサイズを小さくする(ネットワークのキャパシティを減らす。)  
-> 層の数と層のユニットを減らす

3. 重みを正則化をする  
-> オッカムの剃刀  
-> 単純なモデルが正しいモデル  
-> 大きな重みを使用する場合のコストをネットワークの損失関数に追加する

4. ドロップアウトを追加する  
-> 偶然のパターンを破壊する

正則化コスト
1. L1正則化  
-> 追加されるコスト : 重み係数の絶対値  

1. L2正則化  
-> 追加されるコスト : 重み係数の2乗  
-> 荷重衰退ともいう

```python
from keras import regularizersomodel = models.Sequential()
model.add(layers.Dense(16, kernel_regularizer = regularizers.l2(0.001),
                       activation='relu', input_shape=(10000,)))
model.add(layers.Dense(16, kernel_regularizer = regularizers.l2(0.001),
                       activation='relu', input_shape=(10000,)))
model.add(layers.Dense(1, activation='sigmoid'))

#kerasの様々な正則化項
from keras import regularizers
#L1正則化
regularizers.l1(0.001)
regularizers.l1_l2(l1=0.001, l2=0.001)
```

ドロップアウト
```python
model = models.Sequential()
model.add(layers.Dense(16, activaion='relu', input_shape=(10000,)))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(16, activaion='relu'))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(1, activation='sigmoid'))
```

##### 機械学習のワークフロー
###### 1. 問題を定義し、データセットを作成する
- 入力データ、予測しようとしているものは何か  
-> (例)映画レビューの感情を分類できるのは、映画レビューと感情アノテーションの両方がそれ追っている場合に限られる
- 直面している問題はどのような種類のものか  
-> 二値分類なのか、多クラス分類なのか等

***必ずこの段階で立てるべき仮説***
- 出力は入力から予測できるものと仮定する
- 利用可能なデータの情報利得は、入力と出力の関係を学習するのに十分であると仮定する

非定常問題  
-> (例)ファッションのおすすめエンジン  
-> 一月分のデータで訓練されているとする  
-> 人々の購入する服の種類が季節ごとに変化する  
-> 正しい措置は  
1. 直近のデータでモデルを絶えず再訓練する
2. 問題が定常化している時間の尺度でデータを収集する


###### 2. 成功の指標を選択する
正解率なのか、適合率なのか、再現率なのか


###### 3. 評価プロトコルを決定する
1. ホールドアウト法の検証データセットを確保 : データが十分にある場合

2. k分割交差検証を実行 : ホールドアウト法を確実に行うにはサンプルが少ない場合

3. 反復的なk分割交差検証を実行 : 利用可能なデータの量が少ない場合にモデルの評価をかなり正確に行う

###### 4. データの準備をする
- データのテンソルへのフォーマット
- データを小さな値にスケーリング
- 特徴量によって値の範囲が異なる場合 -> 正規化
- 特徴量エンジニアリング(必要であれば)

###### 5. ベースラインを超える性能のモデルを開発する
統計的検出力(ランダムにやったときの確率)を実現する
1. 最後の層の活性化
2. 損失関数
3. 最適化の設定


|  問題の種類  |  最後の層の活性化関数  | 損失関数 |
| :---: |:--- |:--- |
|二値分類問題|sigmoid|binary_crossentropy|
|多クラス単一ラベル分類|softmax|categorical_crossentropy|
|多クラス多ラベル分類|sigmoid|binary_crossentropy|
|任意の値に対する回帰|none|mse|
|0~1の値に対する回帰|sigmoid|mseまたは<br>binary_crossentropy|

###### 6. 過学習するモデルを開発する
1. 層を追加する
2. それらの層を大きくする
3. 訓練のエポック数を増やす

###### 7. モデルの正則化とハイパーパラメータのチューニング
1. ドロップアウトを追加する
2. 層を追加、削除する(別のアーキテクチャを試してみる という)
3. L1/L2正則化を追加する
4. ハイパーパラメータ(層1つあたりのユニット数やオプティマイザの学習率など)を変更してみる
5. 必要であれば特徴エンジニアリングを試す。新しい特徴量を追加するか、情報利得がなさそうな特徴量を削除する
